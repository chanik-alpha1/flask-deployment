# -*- coding: utf-8 -*-
"""Crop Yield Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fTY5asGRE4roQiTQeZibdBhwiiHozA7d
"""



"""Data Description
In this section, we will look the each and every variable of the dataset for our project.

Clonesize — m2 — The average blueberry clone size in the field

Honeybee — bees/m2/min — Honeybee density in the field

Bumbles — bees/m2/min — Bumblebee density in the field

Andrena — bees/m2/min — Andrena bee density in the field

Osmia — bees/m2/min — Osmia bee density in the field

MaxOfUpperTRange — ℃ —The highest record of the upper band daily air temperature during the bloom season

MinOfUpperTRange — ℃ — The lowest record of the upper band daily air temperature

AverageOfUpperTRange — ℃ — The average of the upper band daily air temperature

MaxOfLowerTRange — ℃ — The highest record of the lower band daily air temperature

MinOfLowerTRange — ℃ — The lowest record of the lower band daily air temperature

AverageOfLowerTRange — ℃ — The average of the lower band daily air temperature

RainingDays — Day — The total number of days during the bloom season, each of which has precipitation larger than zero

AverageRainingDays — Day — The average of rainy days in the entire bloom season

Fruitset — Transitioning time of fruit set

Fruitmass — Mass of the fruit set

Seeds — Number of seeds in fruitset

Yield — Crop yield (A target variable)
"""

!pip install shap
!pip install catboost

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import mutual_info_regression, SelectKBest
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.model_selection import GridSearchCV, RepeatedKFold
from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import sklearn
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
import statsmodels.api as sm
from xgboost import XGBRegressor
import shap
#import lightgbm
from lightgbm import LGBMRegressor
#import catboost
from catboost import CatBoostRegressor

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

url = "/content/drive/MyDrive/Colab_datasets/WildBlueberryPollinationSimulationData.csv"
df = pd.read_csv(url)



variables_df = df.drop('yield', axis=1)
target = df['yield']


# matplotlib subplot for the categorical feature
nominal_df = df[['MaxOfUpperTRange','MinOfUpperTRange','AverageOfUpperTRange','MaxOfLowerTRange',
               'MinOfLowerTRange','AverageOfLowerTRange','RainingDays','AverageRainingDays']]


# # matplotlib subplot technique to plot distribution of bees in our dataset
# plt.figure(figsize=(15,10))
# plt.subplot(2,3,1)
# plt.hist(df['bumbles'])
# plt.title("Histogram of bumbles column")
# plt.subplot(2,3,2)
# plt.hist(df['andrena'])
# plt.title("Histogram of andrena column")
# plt.subplot(2,3,3)
# plt.hist(df['osmia'])
# plt.title("Histogram of osmia column")
# plt.subplot(2,3,4)
# plt.hist(df['clonesize'])
# plt.title("Histogram of clonesize column")
# plt.subplot(2,3,5)
# plt.hist(df['honeybee'])
# plt.title("Histogram of honeybee column")
# plt.show()

mi_score = mutual_info_regression(variables_df, target, n_neighbors=3,random_state=42)
mi_score_df = pd.DataFrame({'columns':variables_df.columns, 'MI_score':mi_score})
#mi_score_df.sort_values(by='MI_score', ascending=False)

#The code below indicates the number of variables that most correlate with the target variable in descending order

#Perform a clustering
# clustering using kmeans algorithm
X_clus = variables_df[['honeybee','osmia','bumbles','andrena']]

# standardize the dataset using standard scaler
scaler = StandardScaler()
scaler.fit(X_clus)
X_new_clus = scaler.transform(X_clus)

# K means clustering
clustering = KMeans(n_clusters=3, random_state=42)
clustering.fit(X_new_clus)
n_cluster = clustering.labels_

# add new feature to feature_Df
variables_df['n_cluster'] = n_cluster
df['n_cluster'] = n_cluster
variables_df['n_cluster'].value_counts()

"""Standardizing the dataset"""

features_set = ['AverageRainingDays','clonesize','AverageOfLowerTRange',
               'AverageOfUpperTRange','honeybee','osmia','bumbles','andrena','n_cluster']

# final dataframe
X = variables_df[features_set]
y = target.round(1)

# train and test dataset to build baseline model using GBT and RFs by scaling the dataset
mx_scaler = MinMaxScaler()
X_scaled = pd.DataFrame(mx_scaler.fit_transform(X))
X_scaled.columns = X.columns

"""### Modeling and Model Evaluation"""

# let's fit the data to the models lie adaboost, gradientboost and random forest
#model_dict = {"ada_boost": AdaBoostRegressor(),
#              "gradient_boost": GradientBoostingRegressor(),
#              "random_forest": RandomForestRegressor(),
#              "xg_boost" : XGBRegressor(),
#              "light_gbm" : LGBMRegressor()

#             }

# Cross value scores of the models
#for key, val in model_dict.items():
#    print(f"cross validation for {key}")
#    score = cross_val_score(val, X_scaled, y, cv=5, scoring='neg_mean_squared_error')
#    mean_score = -np.sum(score)/5
#    sqrt_score = np.sqrt(mean_score)
#    print(sqrt_score)

model_dict = {
    "ada_boost": AdaBoostRegressor(),
    "gradient_boost": GradientBoostingRegressor(),
    "random_forest": RandomForestRegressor(),
    "xg_boost": XGBRegressor(),
    "light_gbm": LGBMRegressor()
}

features_set = ['AverageRainingDays','clonesize','AverageOfLowerTRange',
               'AverageOfUpperTRange','honeybee','osmia','bumbles','andrena','n_cluster']

# final dataframe
X = variables_df[features_set]
y = target.round(1)

# train and test dataset to build baseline model using GBT and RFs by scaling the dataset
mx_scaler = MinMaxScaler()
X_scaled = pd.DataFrame(mx_scaler.fit_transform(X))
X_scaled.columns = X.columns
# Assuming you have 'sqrt_score' values calculated from your data, let's create a sample list of scores.
#sqrt_scores = [10, 20, 30, 40, 50]

results = []

for key, val in model_dict.items():
    score = cross_val_score(val, X_scaled, y, cv=5, scoring='neg_mean_squared_error')
    mean_score = -np.sum(score) / 5
    sqrt_score = int(np.sqrt(mean_score))
    results.append({'Algorithm': key, 'Cross validation value': sqrt_score})

# Creating the pandas DataFrame
dff = pd.DataFrame(results)

# Setting 'key' column as the index
dff.set_index('Algorithm', inplace=True)

# Displaying the DataFrame
#print(dff)



# split the train and test data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# gradient boosting regressor modeling
bgt = GradientBoostingRegressor(random_state=42)
bgt.fit(X_train,y_train)
preds = bgt.predict(X_test)
score = bgt.score(X_train,y_train)
rmse_score = np.sqrt(mean_squared_error(y_test, preds))
r2_score = r2_score(y_test, preds)
#print("RMSE score for gradient boosting algorithm:", rmse_score)
#print("R2 score for the model: ", r2_score*100)

# gradient boosting regressor modeling

# split the train and test data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

bgt = LGBMRegressor(random_state=42)
bgt.fit(X_train,y_train)
preds = bgt.predict(X_test)
score = bgt.score(X_train,y_train)
rmse_score = np.sqrt(mean_squared_error(y_test, preds))
r2_score = r2_score(y_test, preds)
#print("RMSE score gradient boosting machine:", rmse_score)
#print("R2 score for the model: ", r2_score*100)

"""#  Hyperparameter Tuning"""

# K-fold split the dataset
kf = KFold(n_splits = 5, shuffle=True, random_state=0)

# params grid for tuning the hyperparameters
param_grid = {'n_estimators': [100,200,400,500,800],
             'learning_rate': [0.1,0.05,0.3,0.7],
             'min_samples_split': [2,4],
             'min_samples_leaf': [0.1,0.4],
             'max_depth': [3,4,7]
             }

# GBR estimator object
estimator = GradientBoostingRegressor(random_state=42)

# Grid search CV object
clf = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=kf,
                   scoring='neg_mean_squared_error', n_jobs=-1)
clf.fit(X_scaled,y)

# print the best the estimator and params
best_estim = clf.best_estimator_
best_score = clf.best_score_
best_param = clf.best_params_
#print("Best Estimator:", best_estim)
#print("Best score:", np.sqrt(-best_score))

# shaply tree explainer
shap_tree = shap.TreeExplainer(bgt)
shap_values = shap_tree.shap_values(X_test)
#shap.summary_plot(shap_values, X_test)

import joblib

# remove the 'n_cluster' feature from the dataset
X_train_n = X_train.drop('n_cluster', axis=1)
X_test_n = X_test.drop('n_cluster', axis=1)

# train a model for flask API creation =
xgb_model = XGBRegressor(max_depth=9, min_child_weight=7, subsample=1.0)
xgb_model.fit(X_train_n, y_train)
pr = xgb_model.predict(X_test_n)
err = mean_absolute_error(y_test, pr)
rmse_n = np.sqrt(mean_squared_error(y_test, pr))

# after training, save the model using joblib library
joblib.dump(xgb_model, 'wbb_xgb_model2.joblib')
